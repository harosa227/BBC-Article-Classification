{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64be210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The goal of this program is to continue my exploration into Natural Language Processing and try out additional pre-processing\n",
    "#techniques. In my previous NLP project, a logistic regression model was built that classified reviews of prescription drugs.\n",
    "#The reviews were placed into a TF-IDF vectorizer, and classified from there. In this project, I am building a neural net that\n",
    "#takes portions of text from BBC articles and places them into one of three different classes based on the content. In contrast\n",
    "#to my previous project, I will also be employing lemmatization and Principle Component Analysis within this code.\n",
    "\n",
    "#Note: The original dataset can be found here: https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab8d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the modules used for this project\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ec9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the articles labeled as business, entertainment, or politics. \n",
    "\n",
    "#Note: All the files used in this program originally consisted of hundreds of individual .txt files that loaded poorly into \n",
    "#dataframes and were difficult to pre-process. As such, I manually split the files into training and testing folders, and \n",
    "#merged each .txt file into a single massive document using a free tool online to save time.\n",
    "#This tool can be found at the following link: https://it365.gitlab.io/txt-merge/\n",
    "\n",
    "business = pd.read_fwf(r\"C:\\Users\\hecto\\OneDrive\\Documents\\Jupyter Notebook\\BBC Text Data\\training\\merged_business.txt\")\n",
    "entertainment = pd.read_fwf(r\"C:\\Users\\hecto\\OneDrive\\Documents\\Jupyter Notebook\\BBC Text Data\\training\\merged_entertainment.txt\")\n",
    "politics = pd.read_fwf(r\"C:\\Users\\hecto\\OneDrive\\Documents\\Jupyter Notebook\\BBC Text Data\\training\\merged_politics.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c38100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - Array sizes match\n"
     ]
    }
   ],
   "source": [
    "#Converting our dataframes to numpy arrays for convenience when building the master text array. \n",
    "\n",
    "business = business.to_numpy()\n",
    "business = business.flatten()\n",
    "\n",
    "entertainment = entertainment.to_numpy()\n",
    "entertainment = entertainment.flatten()\n",
    "\n",
    "politics = politics.to_numpy()\n",
    "politics = politics.flatten()\n",
    "\n",
    "#Building the master array\n",
    "texts = np.array([])\n",
    "texts = np.append(texts, business, axis=0)\n",
    "texts = np.append(texts, entertainment, axis=0)\n",
    "texts = np.append(texts, politics, axis=0)\n",
    "\n",
    "\n",
    "#Let's verify the master dataset has the correct size by checking it against the sum of the component datasets\n",
    "if (business.shape[0] + entertainment.shape[0] + politics.shape[0]) == texts.shape[0]:\n",
    "    print(\"Success - Array sizes match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4598909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.'\n",
      " nan nan ... nan nan nan] \n",
      "\n",
      "[\"A Christmas tree that can receive text messages has been unveiled at London's Tate Britain art gallery.\"\n",
      " nan nan ... nan nan nan] \n",
      "\n",
      "['Maternity pay for new mothers is to rise by £1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt.'\n",
      " nan nan ... nan nan nan] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Here, we'll examine the content from our various text classes. As can be seen, many NaN values were imported into the\n",
    "#dataframe, meaning we'll need to perform some substantial pre-processing.\n",
    "print(business, \"\\n\")\n",
    "print(entertainment, \"\\n\")\n",
    "print(politics, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d7aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - Array sizes match\n"
     ]
    }
   ],
   "source": [
    "#Here we'll build the labels which correspond to the writing we've stored in the \"texts\" array. We'll create a second array\n",
    "#where we append labels in a way that follows the same order in which the \"texts\" array was built, ensuring the label correctly \n",
    "#corresponds to the text.\n",
    "\n",
    "labels = []\n",
    "\n",
    "for i in range(business.shape[0]):\n",
    "    labels.append(\"Business\")\n",
    "   \n",
    "\n",
    "for i in range(entertainment.shape[0]):\n",
    "    labels.append(\"Entertainment\")\n",
    "   \n",
    "\n",
    "for i in range(politics.shape[0]):\n",
    "    labels.append(\"Politics\")\n",
    "    \n",
    "    \n",
    "#We'll also check the array sizes to ensure a 1-1 correspondence between our predictors and labels.\n",
    "if len(labels) == texts.shape[0]:\n",
    "    print(\"Success - Array sizes match\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a0be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the target labels created, we'll append them to our predictor text, and build a new dataframe to facilitate preprocesing\n",
    "labels = np.array(labels)\n",
    "texts = np.append(texts.reshape(-1,1), labels.reshape(-1,1), axis = 1)\n",
    "\n",
    "texts = pd.DataFrame(texts, columns = ['Text', \"Label\"])\n",
    "texts = texts.dropna().reset_index(drop=True)\n",
    "texts['Label'] = texts['Label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a69786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article:\n",
      "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
      "\n",
      "Cleaned Article:\n",
      "The firm which is now one of the biggest investors in Google benefited from sales of highspeed internet connections and higher advert sales TimeWarner said fourth quarter sales rose to bn from bn Its profits were buoyed by oneoff gains which offset a profit dip at Warner Bros and less users for AOL\n"
     ]
    }
   ],
   "source": [
    "#Here, we'll remove unnecessary tokens which act as noise in the text data to facilitate the learning process\n",
    "print(\"Original Article:\" + \"\\n\" + texts['Text'][1] + \"\\n\")\n",
    "\n",
    "\n",
    "def clean_data(review):\n",
    "    \n",
    "    no_punc = re.sub(r'[^\\w\\s]', '', review)\n",
    "    no_digits = ''.join([i for i in no_punc if not i.isdigit()])\n",
    "    no_dbl_space = re.sub(\"  \", \" \", no_digits)\n",
    "    return(no_dbl_space)\n",
    "\n",
    "texts['Text'] = texts['Text'].astype(str)\n",
    "texts['Text'] = texts['Text'].apply(clean_data)\n",
    "\n",
    "print(\"Cleaned Article:\" + \"\\n\" + texts['Text'][1])\n",
    "\n",
    "#Success! Unnecessary noise has been removed from our text! Next, lets lemmatize our reviews for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2239e19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hecto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized data shape before PCA: (8296, 20121)\n"
     ]
    }
   ],
   "source": [
    "#Here, we lemmatize each token and recombine them into a full string for processing by TF-IDF vectorization \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "vectorized_texts = texts[\"Text\"].to_numpy()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_texts = []\n",
    "\n",
    "for i in range(len(vectorized_texts)):\n",
    "    tokens = word_tokenize(vectorized_texts[i])\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_texts.append(lemmatized_tokens)\n",
    "\n",
    "delimiter = \" \"\n",
    "for i in range(len(lemmatized_texts)):\n",
    "    lemmatized_texts[i] = delimiter.join(lemmatized_texts[i])\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "lemmatized_texts = vectorizer.fit_transform(lemmatized_texts)\n",
    "print(\"Lemmatized data shape before PCA: \" + str(lemmatized_texts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd7246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized data shape after PCA: (8296, 100)\n"
     ]
    }
   ],
   "source": [
    "#Finally, we'll reduce the dimensionality of the data to increase the efficiency of the training process, as well as enable \n",
    "#ourselves to use a simpler ML model. \n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "DimReducer = IncrementalPCA(n_components=100)\n",
    "\n",
    "lemmatized_texts = DimReducer.fit_transform(lemmatized_texts)\n",
    "print(\"Lemmatized data shape after PCA: \" + str(lemmatized_texts.shape))\n",
    "x_train, x_test, y_train, y_test = train_test_split(lemmatized_texts, texts['Label'], test_size = 0.15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77f667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model testing accuracy: 0.8128514056224899\n"
     ]
    }
   ],
   "source": [
    "#At last we train our Neural Net, and observe the test accuracy to see how well the model generalizes to the test data\n",
    "myNN = MLPClassifier(hidden_layer_sizes=(50,30,40), activation='tanh', alpha=0.001,max_iter=300)\n",
    "myNN.fit(x_train, y_train)\n",
    "predicted = myNN.predict(x_test)\n",
    "print(\"Your model testing accuracy: \" + str(accuracy_score(predicted, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91131e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This concludes this exploration in NLP. Whilst it isn't visble from this notebook, I spent many hours experimenting with \n",
    "#various model parameters, including the PCA process, model architecture, learning rate, activation functions, and more. \n",
    "#Throughout this testing, the testing accuracy did not rise much greater than 80%, so I suspect a more complex model\n",
    "#architecture is required to achieve my goal of +90% accuracy. As such, I am planning to revisit this project by building a \n",
    "#recurrent neural net in Keras with activation functions that vary throughout the layers, and seeing how this effects \n",
    "#our predictive capabilities. As always, all suggestions are welcomed, and thanks for taking a look!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
